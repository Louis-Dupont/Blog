<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>How to Fail your AI Project – Like I Did | Louis Dupont</title>
<meta name=keywords content><meta name=description content="A few years ago, I worked on my first Generative AI project: a customer-facing AI assistant. The company had unique customer data and believed AI could transform it into a highly personalized, valuable chatbot.
We built a prototype fast. Our users were excited about the first demo.
Our First Big Mistake
But there was a catch: we had almost no access to real users. With so few user tests, we had to rely on ourselves—we became our own test users."><meta name=author content="Louis Dupont"><link rel=canonical href=https://louis-dupont.github.io/Blog/articles/my-realisation-moment/><link crossorigin=anonymous href=/Blog/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://louis-dupont.github.io/Blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://louis-dupont.github.io/Blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://louis-dupont.github.io/Blog/favicon-32x32.png><link rel=apple-touch-icon href=https://louis-dupont.github.io/Blog/apple-touch-icon.png><link rel=mask-icon href=https://louis-dupont.github.io/Blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://louis-dupont.github.io/Blog/articles/my-realisation-moment/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-GX2HR4QZL5"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GX2HR4QZL5")}</script><meta property="og:url" content="https://louis-dupont.github.io/Blog/articles/my-realisation-moment/"><meta property="og:site_name" content="Louis Dupont"><meta property="og:title" content="How to Fail your AI Project – Like I Did"><meta property="og:description" content="A few years ago, I worked on my first Generative AI project: a customer-facing AI assistant. The company had unique customer data and believed AI could transform it into a highly personalized, valuable chatbot.
We built a prototype fast. Our users were excited about the first demo.
Our First Big Mistake But there was a catch: we had almost no access to real users. With so few user tests, we had to rely on ourselves—we became our own test users."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="articles"><meta property="article:published_time" content="2025-03-12T15:59:10+01:00"><meta property="article:modified_time" content="2025-03-12T15:59:10+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="How to Fail your AI Project – Like I Did"><meta name=twitter:description content="A few years ago, I worked on my first Generative AI project: a customer-facing AI assistant. The company had unique customer data and believed AI could transform it into a highly personalized, valuable chatbot.
We built a prototype fast. Our users were excited about the first demo.
Our First Big Mistake
But there was a catch: we had almost no access to real users. With so few user tests, we had to rely on ourselves—we became our own test users."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"What I’ve Learned","item":"https://louis-dupont.github.io/Blog/articles/"},{"@type":"ListItem","position":2,"name":"How to Fail your AI Project – Like I Did","item":"https://louis-dupont.github.io/Blog/articles/my-realisation-moment/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How to Fail your AI Project – Like I Did","name":"How to Fail your AI Project – Like I Did","description":"A few years ago, I worked on my first Generative AI project: a customer-facing AI assistant. The company had unique customer data and believed AI could transform it into a highly personalized, valuable chatbot.\nWe built a prototype fast. Our users were excited about the first demo.\nOur First Big Mistake But there was a catch: we had almost no access to real users. With so few user tests, we had to rely on ourselves—we became our own test users.\n","keywords":[],"articleBody":"A few years ago, I worked on my first Generative AI project: a customer-facing AI assistant. The company had unique customer data and believed AI could transform it into a highly personalized, valuable chatbot.\nWe built a prototype fast. Our users were excited about the first demo.\nOur First Big Mistake But there was a catch: we had almost no access to real users. With so few user tests, we had to rely on ourselves—we became our own test users.\nWe tried to think, behave, and react like our users. At first, it seemed to work well—for us. We spotted issues, fixed them, and each iteration felt like progress\nThen we put the chatbot in front of real users. They barely noticed the difference.\nWe had solved real issues. But were they the right ones? Not really.\nGuesswork Isn’t Iteration Over the next months, we started seeing patterns in their feedback—but every iteration still felt slow and uncertain.\nDuring the long gaps between user tests, we had no choice but to guess whether our changes would make a real difference.\nWe still didn’t know how to properly evaluate it. We kept wondering: is it really working, or only on our pre-selected examples?\nEventually, our AI delivered that initial “wow” effect. Users tried it—but they didn’t come back.\nTechnically, the AI performed well. But none of that mattered, because it wasn’t aligned with what customers actually needed\nWe thought we were listening carefully to our users. Not enough.\nWe were gathering feedback—but too slowly, and without a clear way to turn it into meaningful changes. That’s what made the difference\nWhat If You’re Optimizing the Wrong Thing? Our feedback loop was painfully slow. At the time, we told ourselves it was out of our control, but looking back, that’s exactly where we should have focused.\nLater, I realized something crucial: even if direct user feedback isn’t always available, you can create a proxy if you choose the right metrics.\nI mean, this sounds great on paper, but it’s much harder in practice. We also tried to set up metrics, but had no idea how to properly do it.\nLet’s dig into it!\nThe hardest part isn’t just building metrics, it’s figuring out which ones actually matter. A metric is only useful if it tightly correlates with actual outcomes.\nThe only reliable way to learn this is by talking to users and looking at your data. Skipping this step almost guarantees you’ll optimize for the wrong goals.\nDon’t assume you know the right metrics upfront. Uncover them through iteration, so they stay grounded in reality.\nYour job isn’t to assume the problem—it’s to uncover it. Unless you are your own ideal customer (ICP: Ideal Customer Profile), you don’t truly feel their pain. Your goal is to learn what actually matters, then design the right solution around it.\nThis isn’t a one-shot process where you define metrics once and move on. Your understanding of success will evolve over time, so you should treat this as an ongoing, iterative process.\nEarly in a project, you’ll spend more time refining what matters, and then less overtime as your understanding deepens.\nHow to Measure What Actually Matters? Alongside gathering direct user feedback, you should identify strong proxies for success.\nThe most reliable metrics are those that can be computed deterministically—meaning they provide clear, objective measurements.\nFor example, suppose user testing reveals that the ideal summary length for your use case is between 300-500 words. While this alone doesn’t guarantee quality, it’s an easy deterministic check that helps filter out summaries that are either too short or too long.\nNow consider a less tangible requirement: “The output should be formal and corporate-friendly.” Unlike length, there’s no direct way to measure this, so how do you approximate it?\nThe best approach is to start with human evaluation. Manually reviewing outputs, identifying recurring patterns, and gradually building an approximation based on real-world examples. It’s a big and complex topic, so I’ll dedicate an article exclusively for this.\nOnce you have a clear idea on what matters, and a reliable way to approximate it, you can iterate much faster, with more confidence on what truly matters!\nAlways remember: Metrics are only useful if they are tightly aligned with real-world success. At any stage, you may be optimizing the wrong thing, or measuring it the wrong way. Metrics are a means to an end, not the end itself, so continuously cross-check them against real-world impact.\nYour Competitive Edge Isn’t in Your Model The real challenge for AI engineers isn’t optimizing models—it’s defining and approximating success. Once you know what success looks like and can measure it reliably, improving your AI becomes surprisingly easy.\nThis is the silent trap that catches many teams. They think they’re improving AI when, in reality, they’re just making it more complex.\nBuilding a complex solution is easy. Building the right system that lets you scale AI quality effectively is not.\nYour model, prompt, and architecture should merely be seen as byproducts of your deep understanding of users and your ability to measure what truly matters.\nHand the same model to a competitor, and they won’t be able to iterate effectively, because they don’t have the deep understanding you built.\nIf you are truly looking for a competitive edge, don’t invest in the byproduct, invest in the factory.\nLearn how I can help →\n","wordCount":"897","inLanguage":"en","datePublished":"2025-03-12T15:59:10+01:00","dateModified":"2025-03-12T15:59:10+01:00","author":{"@type":"Person","name":"Louis Dupont"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://louis-dupont.github.io/Blog/articles/my-realisation-moment/"},"publisher":{"@type":"Organization","name":"Louis Dupont","logo":{"@type":"ImageObject","url":"https://louis-dupont.github.io/Blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://louis-dupont.github.io/Blog/ accesskey=h title="Louis Dupont (Alt + H)">Louis Dupont</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://louis-dupont.github.io/Blog/ title=Home><span>Home</span></a></li><li><a href=https://louis-dupont.github.io/Blog/articles/ title=Insights><span>Insights</span></a></li><li><a href=https://louis-dupont.github.io/Blog/work-with-me/ title=Consulting><span>Consulting</span></a></li></ul></nav></header><main class=main><article class=post-content><p style=display:none>Template: single.html</p><h1 class=post-title>How to Fail your AI Project – Like I Did</h1><p class=post-description></p><div class=post-body><p>A few years ago, I worked on my first Generative AI project: a customer-facing AI assistant. The company had unique customer data and believed AI could transform it into a highly personalized, valuable chatbot.</p><p>We built a prototype fast. Our users were excited about the first demo.</p><h2 id=our-first-big-mistake>Our First Big Mistake</h2><p>But there was a catch: we had almost no access to real users. With so few user tests, we had to rely on ourselves—we became our own test users.</p><p>We tried to think, behave, and react like our users. At first, it seemed to work well—for us. We spotted issues, fixed them, and each iteration felt like progress</p><p>Then we put the chatbot in front of real users. They barely noticed the difference.</p><p>We had solved real issues. But were they the right ones? Not really.</p><h2 id=guesswork-isnt-iteration>Guesswork Isn’t Iteration</h2><p>Over the next months, we started seeing patterns in their feedback—but every iteration still felt slow and uncertain.</p><p>During the long gaps between user tests, we had no choice but to guess whether our changes would make a real difference.</p><p>We still didn&rsquo;t know how to properly evaluate it. We kept wondering: is it really working, or only on our pre-selected examples?</p><p>Eventually, our AI delivered that initial &ldquo;wow&rdquo; effect. Users tried it—but they didn&rsquo;t come back.</p><p>Technically, the AI performed well. But none of that mattered, because it wasn&rsquo;t aligned with what customers actually needed</p><p>We thought we were listening carefully to our users. Not enough.</p><p>We were gathering feedback—but too slowly, and without a clear way to turn it into meaningful changes. That&rsquo;s what made the difference</p><h2 id=what-if-youre-optimizing-the-wrong-thing>What If You’re Optimizing the Wrong Thing?</h2><p>Our feedback loop was painfully slow. At the time, we told ourselves it was out of our control, but looking back, that&rsquo;s exactly where we should have focused.</p><p>Later, I realized something crucial: even if direct user feedback isn&rsquo;t always available, you can create a proxy if you choose the right metrics.</p><p>I mean, this sounds great on paper, but it&rsquo;s much harder in practice. We also tried to set up metrics, but had no idea how to properly do it.</p><p><strong><em>Let&rsquo;s dig into it!</em></strong></p><p>The hardest part isn&rsquo;t just building metrics, it&rsquo;s figuring out which ones actually matter. A metric is only useful if it tightly correlates with actual outcomes.</p><p>The only reliable way to learn this is by talking to users and <strong>looking at your data</strong>. Skipping this step almost guarantees you&rsquo;ll optimize for the wrong goals.</p><p>Don&rsquo;t assume you know the right metrics upfront. Uncover them through iteration, so they stay grounded in reality.</p><blockquote><p>Your job isn&rsquo;t to assume the problem—it&rsquo;s to uncover it. Unless you are your own ideal customer (ICP: Ideal Customer Profile), you don&rsquo;t truly feel their pain. Your goal is to learn what actually matters, then design the right solution around it.</p></blockquote><p>This isn&rsquo;t a one-shot process where you define metrics once and move on. Your understanding of success will evolve over time, so you should treat this as an ongoing, iterative process.</p><p>Early in a project, you&rsquo;ll spend more time refining what matters, and then less overtime as your understanding deepens.</p><h2 id=how-to-measure-what-actually-matters>How to Measure What Actually Matters?</h2><p>Alongside gathering direct user feedback, you should identify strong proxies for success.</p><p>The most reliable metrics are those that can be computed deterministically—meaning they provide clear, objective measurements.</p><p>For example, suppose user testing reveals that the ideal summary length for your use case is between 300-500 words. While this alone doesn&rsquo;t guarantee quality, it&rsquo;s an easy deterministic check that helps filter out summaries that are either too short or too long.</p><p>Now consider a less tangible requirement: &ldquo;The output should be formal and corporate-friendly.&rdquo; Unlike length, there&rsquo;s no direct way to measure this, so how do you approximate it?</p><p>The best approach is to start with human evaluation. Manually reviewing outputs, identifying recurring patterns, and gradually building an approximation based on real-world examples.
<em>It&rsquo;s a big and complex topic, so I&rsquo;ll dedicate an article exclusively for this.</em></p><p>Once you have a clear idea on what matters, and a reliable way to approximate it, you can iterate much faster, with more confidence on what truly matters!</p><blockquote><p>Always remember: Metrics are only useful if they are tightly aligned with real-world success. At any stage, you may be optimizing the wrong thing, or measuring it the wrong way. Metrics are a means to an end, not the end itself, so continuously cross-check them against real-world impact.</p></blockquote><h2 id=your-competitive-edge-isnt-in-your-model>Your Competitive Edge Isn’t in Your Model</h2><p>The real challenge for AI engineers isn&rsquo;t optimizing models—it&rsquo;s defining and approximating success. Once you know what success looks like and can measure it reliably, improving your AI becomes surprisingly easy.</p><p>This is the silent trap that catches many teams. They think they&rsquo;re improving AI when, in reality, they&rsquo;re just making it more complex.</p><p>Building a complex solution is easy. Building the right system that lets you scale AI quality effectively is not.</p><p>Your model, prompt, and architecture should merely be seen as byproducts of your deep understanding of users and your ability to measure what truly matters.</p><p>Hand the same model to a competitor, and they won&rsquo;t be able to iterate effectively, because they don&rsquo;t have the deep understanding you built.</p><p>If you are truly looking for a competitive edge, don&rsquo;t invest in the byproduct, invest in the factory.</p><p><a href=../../work-with-me>Learn how I can help →</a></p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://louis-dupont.github.io/Blog/>Louis Dupont</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>