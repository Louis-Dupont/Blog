<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Unblock Your AI (For Good) | Louis Dupont</title>
<meta name=keywords content><meta name=description content="Many teams I&rsquo;ve worked with start out making rapid progress with their AI - usually a RAG system. At first, iteration is fast. Each tweak makes a visible difference. But then, they hit a plateau.
Changes don&rsquo;t seem to make a clear impact anymore. Some changes even backfire. The AI isn&rsquo;t broken, but it&rsquo;s no longer clear what to fix—or even how to tell if it&rsquo;s improving at all.
When Metrics Stop Making Sense
That&rsquo;s when most teams turn to metrics. And that&rsquo;s where things usually go wrong."><meta name=author content="Louis Dupont"><link rel=canonical href=https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/><link crossorigin=anonymous href=/Blog/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://louis-dupont.github.io/Blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://louis-dupont.github.io/Blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://louis-dupont.github.io/Blog/favicon-32x32.png><link rel=apple-touch-icon href=https://louis-dupont.github.io/Blog/apple-touch-icon.png><link rel=mask-icon href=https://louis-dupont.github.io/Blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-GX2HR4QZL5"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GX2HR4QZL5")}</script><meta property="og:url" content="https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/"><meta property="og:site_name" content="Louis Dupont"><meta property="og:title" content="Unblock Your AI (For Good)"><meta property="og:description" content="Many teams I’ve worked with start out making rapid progress with their AI - usually a RAG system. At first, iteration is fast. Each tweak makes a visible difference. But then, they hit a plateau.
Changes don’t seem to make a clear impact anymore. Some changes even backfire. The AI isn’t broken, but it’s no longer clear what to fix—or even how to tell if it’s improving at all.
When Metrics Stop Making Sense That’s when most teams turn to metrics. And that’s where things usually go wrong."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="articles"><meta property="article:published_time" content="2025-03-18T15:42:04+01:00"><meta property="article:modified_time" content="2025-03-18T15:42:04+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Unblock Your AI (For Good)"><meta name=twitter:description content="Many teams I&rsquo;ve worked with start out making rapid progress with their AI - usually a RAG system. At first, iteration is fast. Each tweak makes a visible difference. But then, they hit a plateau.
Changes don&rsquo;t seem to make a clear impact anymore. Some changes even backfire. The AI isn&rsquo;t broken, but it&rsquo;s no longer clear what to fix—or even how to tell if it&rsquo;s improving at all.
When Metrics Stop Making Sense
That&rsquo;s when most teams turn to metrics. And that&rsquo;s where things usually go wrong."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"What I've Learned","item":"https://louis-dupont.github.io/Blog/articles/"},{"@type":"ListItem","position":2,"name":"Unblock Your AI (For Good)","item":"https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Unblock Your AI (For Good)","name":"Unblock Your AI (For Good)","description":"Many teams I\u0026rsquo;ve worked with start out making rapid progress with their AI - usually a RAG system. At first, iteration is fast. Each tweak makes a visible difference. But then, they hit a plateau.\nChanges don\u0026rsquo;t seem to make a clear impact anymore. Some changes even backfire. The AI isn\u0026rsquo;t broken, but it\u0026rsquo;s no longer clear what to fix—or even how to tell if it\u0026rsquo;s improving at all.\nWhen Metrics Stop Making Sense That\u0026rsquo;s when most teams turn to metrics. And that\u0026rsquo;s where things usually go wrong.\n","keywords":[],"articleBody":"Many teams I’ve worked with start out making rapid progress with their AI - usually a RAG system. At first, iteration is fast. Each tweak makes a visible difference. But then, they hit a plateau.\nChanges don’t seem to make a clear impact anymore. Some changes even backfire. The AI isn’t broken, but it’s no longer clear what to fix—or even how to tell if it’s improving at all.\nWhen Metrics Stop Making Sense That’s when most teams turn to metrics. And that’s where things usually go wrong.\nThey set up a dashboard, plug in well-known evaluation metrics like faithfulness, informativeness, or completeness, and start tracking numbers.\nThe model is hitting good scores. But… they’re still lacking the clarity they were looking for.\nOur previous model scored 4.2 in informativeness, 4.6 in faithfulness, and 4.7 in completeness. The new model scores 4.9 in informativeness, 4.5 in faithfulness, and 4.6 in completeness. Should we go for it?\nAnd the reason is simple: these metrics aren’t actionable. They aren’t tied to real-world success.\nA model could hit 100% faithfulness and still be useless. Why? Because generic metrics don’t account for whether the AI is actually solving the problems your users care about.\nInstead of focusing on some abstract measure of “quality,” the real key to improvement is much simpler: understanding how and why your AI is failing.\nThis is where manual error analysis comes in.\nStop Guessing — Start Working! This isn’t glamorous. It’s not high-tech. And that’s exactly why it works.\nYou need to sit down with your logs, one by one, and look at what’s actually going wrong. It forces you to move beyond vague intuition and translate gut feelings into concrete, fixable problems.\nIt may feel slow at first. But if you do it right, patterns will emerge. And when they do, you’ll see your system’s failures more clearly than any generic metric ever could.\nHere’s how it typically goes:\nFirst, you take a real sample of your logs. Carefully go through each output and try to identify recurring mistakes. Once you do, list them. These are your failure modes.\n“Misunderstands the query” “Refuses to answer the question” “Fails to retrieve the right source” Whatever they are, give them names. Tag at least 50 queries (ideally hundreds).\nNow, you don’t just have a list of failures—you have a clear map of how often each one occurs.\nBut you’re not done yet.\nNext, analyze your user queries. What are they actually asking? What topics keep coming back?\n“Technical Troubleshooting” “Product Questions” “Pricing Questions” The real power comes when you combine these tags into a clear input topic vs. failure mode matrix.\nInput Topic vs Failure Mode Query Misunderstanding Answer Refusal Retrieval Failure % of Queries Troubleshooting 40% 10% 5% 85% Product Questions 10% 10% 5% 16% Pricing Questions 2% 10% 90% 2% % of Error 34% 10% 7% You can now start to prioritize and decide what to do next.\nShould you focus on fixing Troubleshooting first? Minimize Query Misunderstanding across all input topics? Drop support for Pricing Questions, since they’re rare and can be handled without a chatbot?\nWell, that’s a product decision, but you now have all the cards in hand to take it.\nStay Far from Evals At this stage, it’s tempting to start automating error tracking with Evals.\nAnd that’s a mistake.\nEvals work only when failure modes are well-defined and repeat across iterations. Jump in too soon, and you’ll likely end up tracking issues that don’t even matter.\nThere are two major ways premature automation backfires:\nYou measure the wrong thing. If you automate before deeply understanding your failure modes, you’ll end up with weak metrics that mislead you. You measure something that isn’t worth tracking long-term. Some problems disappear after a few iterations. If you automate too early, you waste time tracking noise. Just Do It! It’s work. It’s tedious. And it might feel slow at first.\nBut this is the difference between engineering and guessing.\nWhen you do error analysis right:\n→ You stop chasing abstract improvements and focus on real issues that impact users.\n→ You prioritize fixes effectively, instead of throwing darts in the dark.\n→ You know exactly when and where to automate, rather than wasting effort tracking things that don’t matter.\nAI teams rarely fail because they lack solutions.\nThey fail because they rush to conclusions. Automate the wrong metrics. Optimize in the wrong direction.\nError analysis keeps you from making those mistakes. It teaches you what truly matters.\nIt’s a continuous process—one that, when done right, becomes the foundation for scalable Evals.\nRead my Blog →\nLearn how I can help →\n","wordCount":"774","inLanguage":"en","datePublished":"2025-03-18T15:42:04+01:00","dateModified":"2025-03-18T15:42:04+01:00","author":{"@type":"Person","name":"Louis Dupont"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/"},"publisher":{"@type":"Organization","name":"Louis Dupont","logo":{"@type":"ImageObject","url":"https://louis-dupont.github.io/Blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://louis-dupont.github.io/Blog/ accesskey=h title="Louis Dupont (Alt + H)">Louis Dupont</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://louis-dupont.github.io/Blog/ title=Home><span>Home</span></a></li><li><a href=https://louis-dupont.github.io/Blog/articles/ title=Insights><span>Insights</span></a></li><li><a href=https://louis-dupont.github.io/Blog/work-with-me/ title=Consulting><span>Consulting</span></a></li></ul></nav></header><main class=main><article class=post-content><p style=display:none>Template: single.html</p><h1 class=post-title>Unblock Your AI (For Good)</h1><p class=post-description></p><div class=post-body><p>Many teams I&rsquo;ve worked with start out making rapid progress with their AI - usually a RAG system. At first, iteration is fast. Each tweak makes a visible difference. But then, they hit a plateau.</p><p><strong>Changes don&rsquo;t seem to make a clear impact anymore</strong>. Some changes even backfire. The AI isn&rsquo;t broken, but it&rsquo;s no longer clear what to fix—or even how to tell if it&rsquo;s improving at all.</p><h2 id=when-metrics-stop-making-sense>When Metrics Stop Making Sense</h2><p>That&rsquo;s when most teams turn to metrics. And <strong>that&rsquo;s where things usually go wrong.</strong></p><p>They set up a dashboard, plug in well-known evaluation metrics like faithfulness, informativeness, or completeness, and start tracking numbers.</p><p>The model is hitting good scores. But… <strong>they&rsquo;re still lacking the clarity they were looking for.</strong></p><blockquote><p><em>Our previous model scored 4.2 in informativeness, 4.6 in faithfulness, and 4.7 in completeness. The new model scores 4.9 in informativeness, 4.5 in faithfulness, and 4.6 in completeness. Should we go for it?</em></p></blockquote><p>And the reason is simple: these metrics aren&rsquo;t actionable. They aren&rsquo;t tied to real-world success.</p><p>A model could hit <strong>100% faithfulness and still be useless.</strong> Why? Because generic metrics don&rsquo;t account for whether the AI is actually solving the problems your users care about.</p><p>Instead of focusing on some abstract measure of &ldquo;quality,&rdquo; the real key to improvement is much simpler: understanding how and why your AI is failing.</p><p>This is where <strong>manual error analysis</strong> comes in.</p><h2 id=stop-guessing--start-working>Stop Guessing — Start Working!</h2><p>This isn&rsquo;t glamorous. It&rsquo;s not high-tech. <strong>And that&rsquo;s exactly why it works.</strong></p><p>You need to sit down with your logs, one by one, and look at what&rsquo;s actually going wrong. It forces you to move beyond vague intuition and translate gut feelings into concrete, fixable problems.</p><p>It may feel slow at first. But <strong>if you do it right, patterns will emerge.</strong> And when they do, you&rsquo;ll see your system&rsquo;s failures more clearly than any generic metric ever could.</p><p>Here&rsquo;s how it typically goes:</p><p>First, you take a real sample of your logs. Carefully go through each output and try to identify <strong>recurring mistakes.</strong> Once you do, list them. <strong>These are your failure modes.</strong></p><ul><li><em>&ldquo;Misunderstands the query&rdquo;</em></li><li><em>&ldquo;Refuses to answer the question&rdquo;</em></li><li><em>&ldquo;Fails to retrieve the right source&rdquo;</em></li></ul><p>Whatever they are, give them names. <strong>Tag at least 50 queries (ideally hundreds).</strong></p><p>Now, you don&rsquo;t just have a list of failures—you have a clear map of how often each one occurs.</p><p><strong>But you&rsquo;re not done yet.</strong></p><p>Next, analyze your <strong>user queries.</strong> What are they actually asking? What <strong>topics</strong> keep coming back?</p><ul><li><em>&ldquo;Technical Troubleshooting&rdquo;</em></li><li><em>&ldquo;Product Questions&rdquo;</em></li><li><em>&ldquo;Pricing Questions&rdquo;</em></li></ul><p>The real power comes when you combine these tags into a clear input topic vs. failure mode matrix.</p><table><thead><tr><th><strong>Input Topic vs Failure Mode</strong></th><th><strong>Query Misunderstanding</strong></th><th><strong>Answer Refusal</strong></th><th><strong>Retrieval Failure</strong></th><th><strong>% of Queries</strong></th></tr></thead><tbody><tr><td><strong>Troubleshooting</strong></td><td>40%</td><td>10%</td><td>5%</td><td>85%</td></tr><tr><td><strong>Product Questions</strong></td><td>10%</td><td>10%</td><td>5%</td><td>16%</td></tr><tr><td><strong>Pricing Questions</strong></td><td>2%</td><td>10%</td><td>90%</td><td>2%</td></tr><tr><td><strong>% of Error</strong></td><td>34%</td><td>10%</td><td>7%</td><td></td></tr></tbody></table><p>You can now start to <strong>prioritize and decide what to do next.</strong></p><p>Should you focus on fixing <code>Troubleshooting</code> first? Minimize <code>Query Misunderstanding</code> across all input topics? Drop support for <code>Pricing Questions</code>, since they&rsquo;re rare and can be handled without a chatbot?</p><p>Well, that&rsquo;s a product decision, but <strong>you now have all the cards in hand to take it.</strong></p><h2 id=stay-far-from-evals>Stay Far from Evals</h2><p>At this stage, it&rsquo;s tempting to start automating error tracking with Evals.</p><p><strong>And that&rsquo;s a mistake.</strong></p><p>Evals work only when failure modes are well-defined and repeat across iterations. Jump in too soon, and you&rsquo;ll likely end up tracking issues that don&rsquo;t even matter.</p><p>There are two major ways premature automation backfires:</p><ul><li><strong>You measure the wrong thing.</strong> If you automate before deeply understanding your failure modes, you&rsquo;ll end up with weak metrics that mislead you.</li><li><strong>You measure something that isn&rsquo;t worth tracking long-term.</strong> Some problems disappear after a few iterations. If you automate too early, you waste time tracking noise.</li></ul><h2 id=just-do-it>Just Do It!</h2><p>It&rsquo;s work. It&rsquo;s tedious. And it might feel slow at first.</p><p>But <strong>this is the difference between engineering and guessing.</strong></p><p>When you do error analysis right:<br><strong>→</strong> You stop chasing abstract improvements and focus on real issues that impact users.<br><strong>→</strong> You prioritize fixes effectively, instead of throwing darts in the dark.<br><strong>→</strong> You know exactly when and where to automate, rather than wasting effort tracking things that don&rsquo;t matter.</p><p>AI teams rarely fail because they lack solutions.</p><p>They fail because they rush to conclusions. Automate the wrong metrics. Optimize in the wrong direction.</p><p><strong>Error analysis keeps you from making those mistakes.</strong> It teaches you what truly matters.</p><p>It&rsquo;s a continuous process—one that, <strong>when done right, becomes the foundation for scalable Evals.</strong></p><p><a href=../../articles/>Read my Blog →</a><br><a href=../../work-with-me>Learn how I can help →</a></p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://louis-dupont.github.io/Blog/>Louis Dupont</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>