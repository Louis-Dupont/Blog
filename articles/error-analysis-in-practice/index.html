<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Unblock Your AI Project (For Good) | Louis Dupont</title>
<meta name=keywords content><meta name=description content="Many teams I’ve worked with start off with rapid progress in developping their AI - usually a RAG system. At first, iteration is fast. Each tweak brings noticeable improvements. But then, they hit a plateau.
What used to feel obvious now feels uncertain. Changes don’t seem to make a clear impact anymore. Some even backfire. The system isn&rsquo;t broken, but it’s no longer clear what to fix—or even how to tell if it’s improving at all."><meta name=author content="Louis Dupont"><link rel=canonical href=https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/><link crossorigin=anonymous href=/Blog/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://louis-dupont.github.io/Blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://louis-dupont.github.io/Blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://louis-dupont.github.io/Blog/favicon-32x32.png><link rel=apple-touch-icon href=https://louis-dupont.github.io/Blog/apple-touch-icon.png><link rel=mask-icon href=https://louis-dupont.github.io/Blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-GX2HR4QZL5"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GX2HR4QZL5")}</script><meta property="og:url" content="https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/"><meta property="og:site_name" content="Louis Dupont"><meta property="og:title" content="Unblock Your AI Project (For Good)"><meta property="og:description" content="Many teams I’ve worked with start off with rapid progress in developping their AI - usually a RAG system. At first, iteration is fast. Each tweak brings noticeable improvements. But then, they hit a plateau.
What used to feel obvious now feels uncertain. Changes don’t seem to make a clear impact anymore. Some even backfire. The system isn’t broken, but it’s no longer clear what to fix—or even how to tell if it’s improving at all."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="articles"><meta property="article:published_time" content="2025-03-18T15:42:04+01:00"><meta property="article:modified_time" content="2025-03-18T15:42:04+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Unblock Your AI Project (For Good)"><meta name=twitter:description content="Many teams I’ve worked with start off with rapid progress in developping their AI - usually a RAG system. At first, iteration is fast. Each tweak brings noticeable improvements. But then, they hit a plateau.
What used to feel obvious now feels uncertain. Changes don’t seem to make a clear impact anymore. Some even backfire. The system isn&rsquo;t broken, but it’s no longer clear what to fix—or even how to tell if it’s improving at all."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Thoughts and Learnings","item":"https://louis-dupont.github.io/Blog/articles/"},{"@type":"ListItem","position":2,"name":"Unblock Your AI Project (For Good)","item":"https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Unblock Your AI Project (For Good)","name":"Unblock Your AI Project (For Good)","description":"Many teams I’ve worked with start off with rapid progress in developping their AI - usually a RAG system. At first, iteration is fast. Each tweak brings noticeable improvements. But then, they hit a plateau.\nWhat used to feel obvious now feels uncertain. Changes don’t seem to make a clear impact anymore. Some even backfire. The system isn\u0026rsquo;t broken, but it’s no longer clear what to fix—or even how to tell if it’s improving at all.\n","keywords":[],"articleBody":"Many teams I’ve worked with start off with rapid progress in developping their AI - usually a RAG system. At first, iteration is fast. Each tweak brings noticeable improvements. But then, they hit a plateau.\nWhat used to feel obvious now feels uncertain. Changes don’t seem to make a clear impact anymore. Some even backfire. The system isn’t broken, but it’s no longer clear what to fix—or even how to tell if it’s improving at all.\nWhen Metrics Stop Making Sense That’s when most teams turn to metrics. And that’s where things usually go wrong.\nThey set up a dashboard, plug in well-known evaluation metrics—faithfulness, informativeness, recall—and start tracking numbers.\nThe model is hitting good scores. And yet… they’re still lacking the clarity they were looking for.\nOur previous model scored 4.3 in informativeness, 4.6 in faithfulness, and 4.7 in completeness. The new model scores 4.6 in informativeness, 4.5 in faithfulness, and 4.6 in completeness. Should we go for it?\nAnd the reason is simple: these metrics aren’t actionable. They aren’t tied to real-world success.\nA model could hit 100% faithfulness and still be useless. Why? Because generic metrics don’t account for whether the AI is actually solving the problems your users care about.\nInstead of focusing on some abstract measure of “quality,” the real key to improvement is much simpler: understanding how and why your AI is failing.\nThis is where manual error analysis comes in.\nStop Guessing — Start Working! This isn’t glamorous. It’s not high-tech. And that’s exactly why it works.\nYou need to sit down with your logs, one by one, and look at what’s actually going wrong. It forces you to move beyond vague intuition and translate gut feelings into concrete, fixable problems.\nIt may feel slow at first. But if you do it right, patterns will emerge. And when they do, you’ll see your system’s failures more clearly than any generic metric ever could.\nHere’s how it typically goes:\nFirst, you take a real sample of your logs. Carefully go through each output carefully and try to identify recurring mistakes. Once you do, list them. These are the Failure Modes.\n“Misunderstands the query” “Refuses to answer the question” “Fails to retrieve the right source” Whatever they are, give them names. Tag at least 50 queries (ideally hundreds).\nNow, you don’t just have a list of failures—you have a map of how often each problem happens.\nBut you’re not done yet.\nNow, do the same for your user queries. What are they actually asking? What topics keep coming back?\n“Technical Troubleshooting” “Product Questions” “Pricing Questions” The beauty comes when you combine these tags into a clear matrix - topic vs. failure modes.\nInput Topic vs Failure Mode Query missunderstanding Answer Refusal Retrievial Failure % of Queries Troubleshooting 40% 10% 5% 85% Product Questions 10% 10% 5% 16% Pricing Questions 2% 10% 90% 2% % of Error 34% 10% 7% 100% You can now start to prioritize and decide what to do next.\nShould you optimize for a specific input topic ? Or failure mode? Support only 2 out of 3 topics ?\nWell, that’s a product decision, but you now have all the cards in hand to take it and decide on what to prioritize.\nStay Far from Eval At this stage, it’s tempting to start automating error tracking with Evals.\nAnd that’s a mistake.\nEvals work only when failure modes are well-defined and repeat across iterations. If you jump in too soon, you will probably end up tracking issues that aren’t worth measuring.\nThere are two major ways premature automation backfires:\nYou measure the wrong thing. If you automate before deeply understanding your failure modes, and end up with a weak metrics that mislead you. You measure something that isn’t worth tracking long-term. Some problems disappear after a few iterations. If you automate too early, you waste time tracking noise. This is why manual analysis comes first. Only automate when a failure mode proves to be recurring across multiple iterations. Otherwise, you’re just losing time adding unneeded complexity.\nExclusive Bonus There’s another key benefit to doing error analysis right: you discover when a single approach isn’t enough.\nMaybe a retrieval tweak boosts retrievial quality for pricing queries—but makes it worse for product questions.\nMaybe a prompt change helps customer service requests—but affects technical answers.\nIf fixing one input topic thing hurts another, that’s a clear sign that you shouldn’t be trying to force a single solution. You need routing.\nAnd here’s the good news: you’ve already done the hard work.\nSince you’ve tagged and categorized failures, you already know which types of queries struggle with which failure modes. Now, you can design a simple routing system:\nWhen a query is about pricing, send it through a pricing-optimized prompt. When it’s a troubleshooting request, adjust retrieval settings to favor technical accuracy. Each query type gets a specialized pathway, rather than forcing one-size-fits-all responses.\nThis approach massively boosts accuracy—because instead of making trade-offs, you’re optimizing each case individually.\nJust Do It! This is work. It’s tedious. And it might feel slow at first.\nBut this is the difference between engineering and guessing.\nWhen you do error analysis right:\n✅ You stop chasing abstract improvements and focus on real issues that impact users.\n✅ You prioritize fixes effectively, instead of throwing darts in the dark.\n✅ You know exactly when and where to automate, rather than wasting effort tracking things that don’t matter.\nMost AI teams don’t fail because they lack good models.\nThey fail because they lack a clear, systematic way to improve them.\nError analysis is the cornerstone of that system.\n","wordCount":"936","inLanguage":"en","datePublished":"2025-03-18T15:42:04+01:00","dateModified":"2025-03-18T15:42:04+01:00","author":{"@type":"Person","name":"Louis Dupont"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://louis-dupont.github.io/Blog/articles/error-analysis-in-practice/"},"publisher":{"@type":"Organization","name":"Louis Dupont","logo":{"@type":"ImageObject","url":"https://louis-dupont.github.io/Blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://louis-dupont.github.io/Blog/ accesskey=h title="Louis Dupont (Alt + H)">Louis Dupont</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://louis-dupont.github.io/Blog/ title=Home><span>Home</span></a></li><li><a href=https://louis-dupont.github.io/Blog/articles/ title=Articles><span>Articles</span></a></li><li><a href=https://louis-dupont.github.io/Blog/work-with-me/ title="Work with me"><span>Work with me</span></a></li></ul></nav></header><main class=main><article class=post-content><p style=display:none>Template: single.html</p><h1 class=post-title>Unblock Your AI Project (For Good)</h1><p class=post-description></p><div class=post-body><p>Many teams I’ve worked with start off with rapid progress in developping their AI - usually a RAG system. At first, iteration is fast. Each tweak brings noticeable improvements. But then, they hit a plateau.</p><p>What used to feel obvious now feels uncertain. Changes don’t seem to make a clear impact anymore. Some even backfire. The system isn&rsquo;t broken, but it’s no longer clear what to fix—or even how to tell if it’s improving at all.</p><h2 id=when-metrics-stop-making-sense>When Metrics Stop Making Sense</h2><p>That’s when most teams turn to metrics. And that’s where things usually go wrong.</p><p>They set up a dashboard, plug in well-known evaluation metrics—faithfulness, informativeness, recall—and start tracking numbers.</p><p>The model is hitting good scores. And yet… they&rsquo;re still lacking the clarity they were looking for.</p><blockquote><p><em>Our previous model scored 4.3 in informativeness, 4.6 in faithfulness, and 4.7 in completeness. The new model scores 4.6 in informativeness, 4.5 in faithfulness, and 4.6 in completeness. Should we go for it?</em></p></blockquote><p>And the reason is simple: these metrics aren’t actionable. They aren’t tied to real-world success.</p><p>A model could hit 100% faithfulness and still be useless. Why? Because generic metrics don’t account for whether the AI is actually solving the problems your users care about.</p><p>Instead of focusing on some abstract measure of &ldquo;quality,&rdquo; the real key to improvement is much simpler: understanding how and why your AI is failing.</p><p>This is where manual error analysis comes in.</p><h2 id=stop-guessing--start-working>Stop Guessing — Start Working!</h2><p>This isn’t glamorous. It’s not high-tech. And that’s exactly why it works.</p><p>You need to sit down with your logs, one by one, and look at what’s actually going wrong. It forces you to move beyond vague intuition and translate gut feelings into concrete, fixable problems.</p><p>It may feel slow at first. But if you do it right, patterns will emerge. And when they do, you’ll see your system’s failures more clearly than any generic metric ever could.</p><p>Here’s how it typically goes:</p><p><strong>First, you take a real sample of your logs</strong>. Carefully go through each output carefully and try to identify recurring mistakes. Once you do, list them. These are the Failure Modes.</p><ul><li>&ldquo;Misunderstands the query&rdquo;</li><li>&ldquo;Refuses to answer the question&rdquo;</li><li>&ldquo;Fails to retrieve the right source&rdquo;</li></ul><p>Whatever they are, give them names. Tag at least 50 queries (ideally hundreds).</p><p>Now, you don’t just have a list of failures—you have a map of how often each problem happens.</p><p><strong>But you’re not done yet.</strong></p><p>Now, do the same for your user queries. What are they actually asking? What <strong>topics</strong> keep coming back?</p><ul><li>&ldquo;Technical Troubleshooting&rdquo;</li><li>&ldquo;Product Questions&rdquo;</li><li>&ldquo;Pricing Questions&rdquo;</li></ul><p>The beauty comes when you combine these tags into a clear matrix - topic vs. failure modes.</p><table><thead><tr><th>Input Topic vs Failure Mode</th><th>Query missunderstanding</th><th>Answer Refusal</th><th>Retrievial Failure</th><th>% of Queries</th></tr></thead><tbody><tr><td>Troubleshooting</td><td>40%</td><td>10%</td><td>5%</td><td>85%</td></tr><tr><td>Product Questions</td><td>10%</td><td>10%</td><td>5%</td><td>16%</td></tr><tr><td>Pricing Questions</td><td>2%</td><td>10%</td><td>90%</td><td>2%</td></tr><tr><td>% of Error</td><td>34%</td><td>10%</td><td>7%</td><td>100%</td></tr></tbody></table><p>You can now start to prioritize and decide what to do next.</p><p>Should you optimize for a specific input topic ? Or failure mode? Support only 2 out of 3 topics ?</p><p>Well, that&rsquo;s a product decision, but you now have all the cards in hand to take it and decide on what to prioritize.</p><h2 id=stay-far-from-eval>Stay Far from Eval</h2><p>At this stage, it’s tempting to start automating error tracking with Evals.</p><p>And that’s a mistake.</p><p>Evals work only when failure modes are <strong>well-defined</strong> and <strong>repeat across iterations</strong>. If you jump in too soon, you will probably end up tracking issues that aren’t worth measuring.</p><p>There are two major ways premature automation backfires:</p><ul><li><strong>You measure the wrong thing</strong>. If you automate before deeply understanding your failure modes, and end up with a weak metrics that mislead you.</li><li><strong>You measure something that isn’t worth tracking long-term</strong>. Some problems disappear after a few iterations. If you automate too early, you waste time tracking noise.</li></ul><p>This is why <strong>manual analysis comes first</strong>. Only automate when a failure mode proves to be recurring across multiple iterations. Otherwise, you’re just losing time adding unneeded complexity.</p><h2 id=exclusive-bonus>Exclusive Bonus</h2><p>There’s another key benefit to doing error analysis right: <strong>you discover when a single approach isn’t enough.</strong></p><p>Maybe a retrieval tweak boosts retrievial quality for pricing queries—but makes it worse for product questions.</p><p>Maybe a prompt change helps customer service requests—but affects technical answers.</p><p><strong>If fixing one input topic thing hurts another</strong>, that’s a clear sign that you shouldn’t be trying to force a single solution. <strong>You need routing</strong>.</p><p>And here’s the good news: you’ve already done the hard work.</p><p>Since you’ve tagged and categorized failures, you already know which types of queries struggle with which failure modes. Now, you can design a simple routing system:</p><ul><li>When a query is about pricing, send it through a pricing-optimized prompt.</li><li>When it’s a troubleshooting request, adjust retrieval settings to favor technical accuracy.</li></ul><p>Each query type gets a specialized pathway, rather than forcing one-size-fits-all responses.</p><p>This approach massively boosts accuracy—because instead of making trade-offs, you’re optimizing each case individually.</p><h2 id=just-do-it>Just Do It!</h2><p>This is work. It’s tedious. And it might feel slow at first.</p><p>But this is the difference between engineering and guessing.</p><p>When you do error analysis right:</p><p>✅ You stop chasing abstract improvements and focus on real issues that impact users.</p><p>✅ You prioritize fixes effectively, instead of throwing darts in the dark.</p><p>✅ You know exactly when and where to automate, rather than wasting effort tracking things that don’t matter.</p><p>Most AI teams don’t fail because they lack good models.</p><p>They fail because they lack a clear, systematic way to improve them.</p><p>Error analysis is the cornerstone of that system.</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://louis-dupont.github.io/Blog/>Louis Dupont</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>