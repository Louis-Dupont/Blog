<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>How to Build Evals That Drive Impact | Louis Dupont</title>
<meta name=keywords content><meta name=description content="When you set up evaluation, you’re not just adding a score.
You’re building a system that decides what &ldquo;good&rdquo; looks like.
Most teams don’t treat it that seriously. They build evals by convenience. Automate too early. Use generic metrics. Plug in LLM-as-a-Judge with a vague prompt and hope it works.
It looks scientific, but in practice it’s just noise. Eventually they end up with an AI that does great on their internal benchmark but that none of their users really sticks to."><meta name=author content="Louis Dupont"><link rel=canonical href=https://louis-dupont.github.io/Blog/articles/how-to-build-evals-that-drive-impact/><link crossorigin=anonymous href=/Blog/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://louis-dupont.github.io/Blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://louis-dupont.github.io/Blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://louis-dupont.github.io/Blog/favicon-32x32.png><link rel=apple-touch-icon href=https://louis-dupont.github.io/Blog/apple-touch-icon.png><link rel=mask-icon href=https://louis-dupont.github.io/Blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://louis-dupont.github.io/Blog/articles/how-to-build-evals-that-drive-impact/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-GX2HR4QZL5"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GX2HR4QZL5")}</script><meta property="og:url" content="https://louis-dupont.github.io/Blog/articles/how-to-build-evals-that-drive-impact/"><meta property="og:site_name" content="Louis Dupont"><meta property="og:title" content="How to Build Evals That Drive Impact"><meta property="og:description" content="When you set up evaluation, you’re not just adding a score.
You’re building a system that decides what “good” looks like.
Most teams don’t treat it that seriously. They build evals by convenience. Automate too early. Use generic metrics. Plug in LLM-as-a-Judge with a vague prompt and hope it works.
It looks scientific, but in practice it’s just noise. Eventually they end up with an AI that does great on their internal benchmark but that none of their users really sticks to."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="articles"><meta property="article:published_time" content="2025-04-01T16:32:04+01:00"><meta property="article:modified_time" content="2025-04-01T16:32:04+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="How to Build Evals That Drive Impact"><meta name=twitter:description content="When you set up evaluation, you’re not just adding a score.
You’re building a system that decides what &ldquo;good&rdquo; looks like.
Most teams don’t treat it that seriously. They build evals by convenience. Automate too early. Use generic metrics. Plug in LLM-as-a-Judge with a vague prompt and hope it works.
It looks scientific, but in practice it’s just noise. Eventually they end up with an AI that does great on their internal benchmark but that none of their users really sticks to."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"What I've Learned","item":"https://louis-dupont.github.io/Blog/articles/"},{"@type":"ListItem","position":2,"name":"How to Build Evals That Drive Impact","item":"https://louis-dupont.github.io/Blog/articles/how-to-build-evals-that-drive-impact/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How to Build Evals That Drive Impact","name":"How to Build Evals That Drive Impact","description":"When you set up evaluation, you’re not just adding a score.\nYou’re building a system that decides what \u0026ldquo;good\u0026rdquo; looks like.\nMost teams don’t treat it that seriously. They build evals by convenience. Automate too early. Use generic metrics. Plug in LLM-as-a-Judge with a vague prompt and hope it works.\nIt looks scientific, but in practice it’s just noise. Eventually they end up with an AI that does great on their internal benchmark but that none of their users really sticks to.\n","keywords":[],"articleBody":"When you set up evaluation, you’re not just adding a score.\nYou’re building a system that decides what “good” looks like.\nMost teams don’t treat it that seriously. They build evals by convenience. Automate too early. Use generic metrics. Plug in LLM-as-a-Judge with a vague prompt and hope it works.\nIt looks scientific, but in practice it’s just noise. Eventually they end up with an AI that does great on their internal benchmark but that none of their users really sticks to.\nWhy does this happen ? They skip the part where they actually learn what matters.\nSo before going further, quick sanity check:\nDid you identify and name the recurring ways your system fails? Have you already scored your AI manually? If not, you’re not ready to build evals yet. Start here instead → Learn to Systematically Improve your AI\nFor everyone else, let’s fix the way you scale how you evaluate.\nWhat If You Could Scale Yourself? Let’s say you already did the work. You looked at logs. You tagged outputs. You saw patterns. You built a clear view of where your model fails, and what a good answer should look like.\nGreat. Now comes the bottleneck: you can’t do that at scale.\nIt doesn’t matter if your intuition is right if it’s not repeatable.\nSo here’s the real goal: turn your own judgment into a repeatable process. You’re not looking for an hypothetical best metric. You’re teaching the system to approximate what you already know matters.\nAnd the best way to do that? Train an evaluator to mirror your decisions. Not someone else’s framework. Not some generic leaderboard metric. Just you, scaled.\nLet’s walk through how to do that with an LLM.\nStructure the Evaluation Start with your evaluation vocabulary. If you’re using continuous scores (like 1–100), you’re asking for trouble. LLMs don’t do well with regression. And even humans struggle to be consistent there.\nYou can work with a set of categories, but the best format remains boolean labels. Pass/Fail, for each failure mode.\nNot “score from 1 to 5”, but “was the Booking successfull?”, “is it too verbose?”, “did it miss the user’s intent?” Each with a yes/no answer.\nThis forces clarity. And it’s far easier to match with a structured output later.\nIf you’ve already labeled your data this way, you’re ahead.\nIf not, I encourage you to do so. You can check my articles that talks about it → Learn to Systematically Improve your AI\nOnboard your Judge Now that you have structured labels, it’s time to build the evaluator.\nUse an LLM, but treat it like onboarding a new teammate. You wouldn’t give them a checklist and hope for the best. You’d show examples, walk through edge cases, and explain what good looks like.\nThat’s your prompt.\nTip: Structured outputs help here. Force the output to match the format of your labels.\nCheck Alignment This is where most teams get lazy. You ask the Judge to score the same examples you already scored. Then you check: how often do you agree?\nLook at where you disagree. Are the disagreements meaningful? Are they edge cases? Or signs that your judge just doesn’t get it?\nYou don’t need perfect overlap. But you do need predictable alignment. If you and the Judge are in sync most of the time, that’s enough to trust it for early filtering.\nAs you improve your Judge, you’ll start to see your own criteria more clearly. You’ll notice which patterns are easy to encode, and which are fuzzy. You might even revise what you thought “good” meant.\nIt means you’re not only building an eval, you’re also refining understanding.\nTest Generalisation So it agrees with you on the data you gave it. Great. But how well will it do on new samples?\nHow do you know you didn’t just select the Judge that just does great on the exam materials but didn’t learn your reasoning?\nYou need to check.\nTake more samples that were not used to evaluate the Judge - if needed annotate more - and check how well it does.\nOverfitting isn’t just a model problem. It happens when you unconsciously tune your prompts too tightly around a specific dataset.\nEven if your Judge agrees with you on your sample, it might still fail on new samples.\nTo check that, take a fresh set of examples - ideally fresh from production - and score them manually. Then compare with the Judge’s output.\nThis is your holdout set. It tells you if you’ve overfit the evaluator to your sample. If it doesn’t generalise, revisit your prompt, your labels, or your definition of “good.” You also may want to add more samples to make it more robust to overfitting.\nWhat You’re Actually Doing At this point, it’s worth being clear about what’s happening.\nYou’re not just “setting up evals.”\nYou’re training a classifier.\nThe method is prompting, not fine-tuning. But the logic is the same: supervised learning.\nThat means the usual rules apply.\nGarbage in, garbage out Overfitting happens fast Bad labels lead to bad models Once you see it like this, it becomes obvious why most evaluation setups collapse. They’re doing machine learning without treating it as such.\nIf you’re hitting limitations, you can go beyond LLM-as-a-Judge. Fine-tune a small classifier. Train a discriminative model. But for most teams, that’s overkill.\nFrom Clarity to Speed When your evaluator works - when the proxy you’ve built reflects your actual judgment - you unlock speed.\nYou can filter outputs at scale. Detect regressions automatically. Run batch experiments and know what matters. Align your team on a quantitative definition of quality. Stop relying on gut feel.\nYou don’t need to track every edge case. But you do need to trust that your eval reflects something real.\nThat’s the difference between guessing and engineering.\nThat’s how you go from “vibe check” to deliberate iteration, so you can scale with speed and confidence.\nRead my Blog →\nLearn how I can help →\n","wordCount":"999","inLanguage":"en","datePublished":"2025-04-01T16:32:04+01:00","dateModified":"2025-04-01T16:32:04+01:00","author":{"@type":"Person","name":"Louis Dupont"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://louis-dupont.github.io/Blog/articles/how-to-build-evals-that-drive-impact/"},"publisher":{"@type":"Organization","name":"Louis Dupont","logo":{"@type":"ImageObject","url":"https://louis-dupont.github.io/Blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://louis-dupont.github.io/Blog/ accesskey=h title="Louis Dupont (Alt + H)">Louis Dupont</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://louis-dupont.github.io/Blog/ title=Home><span>Home</span></a></li><li><a href=https://louis-dupont.github.io/Blog/articles/ title=Insights><span>Insights</span></a></li><li><a href=https://louis-dupont.github.io/Blog/work-with-me/ title=Consulting><span>Consulting</span></a></li></ul></nav></header><main class=main><article class=post-content><p style=display:none>Template: single.html</p><h1 class=post-title>How to Build Evals That Drive Impact</h1><p class=post-description></p><div class=post-body><p>When you set up evaluation, you’re not just adding a score.</p><p>You’re building a <strong>system that decides what &ldquo;good&rdquo; looks like</strong>.</p><p>Most teams don’t treat it that seriously. They build evals by convenience. Automate too early. Use generic metrics. Plug in LLM-as-a-Judge with a vague prompt and hope it works.</p><p>It looks scientific, but in practice it’s just noise. Eventually they end up with an AI that does great on their internal benchmark but that none of their users really sticks to.</p><p>Why does this happen ? They skip the part where they actually learn what matters.</p><p>So before going further, quick sanity check:</p><ul><li>Did you identify and name the recurring ways your system fails?</li><li>Have you already scored your AI manually?</li></ul><p>If not, you’re not ready to build evals yet. Start here instead → <a href=../../articles/learn-to-systematically-improve-your-ai>Learn to Systematically Improve your AI</a></p><p>For everyone else, let’s fix the way you scale how <strong><em>you</em></strong> evaluate.</p><h2 id=what-if-you-could-scale-yourself>What If You Could Scale <em>Yourself</em>?</h2><p>Let’s say you already did the work. You looked at logs. You tagged outputs. You saw patterns. You built a clear view of where your model fails, and what a good answer should look like.</p><p>Great. Now comes the bottleneck: you can’t do that at scale.</p><p>It doesn’t matter if your intuition is right if it’s not repeatable.</p><p>So here’s the real goal: <strong>turn your own judgment into a repeatable process</strong>. You’re not looking for an hypothetical best metric. You’re teaching the system to approximate what <em>you</em> already know matters.</p><p>And the best way to do that? Train an evaluator to mirror your decisions. Not someone else&rsquo;s framework. Not some generic leaderboard metric. Just <em>you</em>, scaled.</p><p>Let’s walk through how to do that with an LLM.</p><h2 id=structure-the-evaluation>Structure the Evaluation</h2><p>Start with your evaluation vocabulary. If you’re using continuous scores (like 1–100), you’re asking for trouble. LLMs don’t do well with regression. And even humans struggle to be consistent there.</p><p>You can work with a set of categories, but the best format remains boolean labels. Pass/Fail, for each failure mode.</p><p>Not “score from 1 to 5”, but &ldquo;was the Booking successfull?&rdquo;, &ldquo;is it too verbose?&rdquo;, &ldquo;did it miss the user&rsquo;s intent?&rdquo; Each with a yes/no answer.</p><p>This forces clarity. And it’s far easier to match with a structured output later.</p><p>If you’ve already labeled your data this way, you’re ahead.</p><p>If not, I encourage you to do so. You can check my articles that talks about it → <a href=../../articles/learn-to-systematically-improve-your-ai>Learn to Systematically Improve your AI</a></p><h2 id=onboard-your-judge>Onboard your Judge</h2><p>Now that you have structured labels, it’s time to build the evaluator.</p><p>Use an LLM, but treat it like onboarding a new teammate. You wouldn’t give them a checklist and hope for the best. You’d show examples, walk through edge cases, and explain what good looks like.</p><p>That’s your prompt.</p><blockquote><p>Tip: Structured outputs help here. Force the output to match the format of your labels.</p></blockquote><h2 id=check-alignment>Check Alignment</h2><p>This is where most teams get lazy. You ask the Judge to score the same examples you already scored. Then you check: how often do you agree?</p><p>Look at <em>where</em> you disagree. Are the disagreements meaningful? Are they edge cases? Or signs that your judge just doesn’t get it?</p><p>You don’t need perfect overlap. But you do need <em>predictable alignment</em>. If you and the Judge are in sync most of the time, that’s enough to trust it for early filtering.</p><blockquote><p>As you improve your Judge, you’ll start to see your own criteria more clearly. You’ll notice which patterns are easy to encode, and which are fuzzy. You might even revise what you thought &ldquo;good&rdquo; meant.</p><p>It means you’re not only building an eval, you’re also refining understanding.</p></blockquote><h2 id=test-generalisation>Test Generalisation</h2><p>So it agrees with you on the data you gave it. Great. But how well will it do on new samples?</p><p>How do you know you didn’t just select the Judge that just does great on the exam materials but didn’t learn your reasoning?</p><p>You need to check.</p><p>Take more samples that were not used to evaluate the Judge - if needed annotate more - and check how well it does.</p><blockquote><p>Overfitting isn’t just a model problem. It happens when you unconsciously tune your prompts too tightly around a specific dataset.</p></blockquote><p>Even if your Judge agrees with you on your sample, it might still fail on new samples.</p><p>To check that, take a fresh set of examples - ideally fresh from production - and score them manually. Then compare with the Judge’s output.</p><p>This is your holdout set. It tells you if you’ve overfit the evaluator to your sample. If it doesn’t generalise, revisit your prompt, your labels, or your definition of “good.” You also may want to add more samples to make it more robust to overfitting.</p><h2 id=what-youre-actually-doing>What You’re Actually Doing</h2><p>At this point, it’s worth being clear about what’s happening.</p><p>You’re not just “setting up evals.”</p><p>You’re training a classifier.</p><p>The method is prompting, not fine-tuning. But the logic is the same: supervised learning.</p><p>That means the usual rules apply.</p><ul><li>Garbage in, garbage out</li><li>Overfitting happens fast</li><li>Bad labels lead to bad models</li></ul><p>Once you see it like this, it becomes obvious why most evaluation setups collapse. They’re doing machine learning without treating it as such.</p><blockquote><p>If you&rsquo;re hitting limitations, you can go beyond LLM-as-a-Judge. Fine-tune a small classifier. Train a discriminative model. But for most teams, that’s overkill.</p></blockquote><h2 id=from-clarity-to-speed>From Clarity to Speed</h2><p>When your evaluator works - when the proxy you’ve built reflects your actual judgment - you unlock speed.</p><p>You can filter outputs at scale. Detect regressions automatically. Run batch experiments and know what matters. Align your team on a quantitative definition of quality. Stop relying on gut feel.</p><p>You don’t need to track every edge case. But you do need to trust that your eval reflects something real.</p><p>That’s the difference between guessing and engineering.</p><p>That’s how you go from “vibe check” to deliberate iteration, so you can scale with speed <em>and confidence</em>.</p><p><a href=../../articles/>Read my Blog →</a><br><a href=../../work-with-me>Learn how I can help →</a></p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://louis-dupont.github.io/Blog/>Louis Dupont</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>