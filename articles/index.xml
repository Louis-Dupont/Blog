<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>What I've Learned on Louis Dupont</title><link>https://louis-dupont.github.io/Blog/articles/</link><description>Recent content in What I've Learned on Louis Dupont</description><generator>Hugo -- 0.145.0</generator><language>en-us</language><lastBuildDate>Tue, 01 Apr 2025 16:32:04 +0100</lastBuildDate><atom:link href="https://louis-dupont.github.io/Blog/articles/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Build Evals That Drive Impact</title><link>https://louis-dupont.github.io/Blog/articles/how-to-build-evals-that-drive-impact/</link><pubDate>Tue, 01 Apr 2025 16:32:04 +0100</pubDate><guid>https://louis-dupont.github.io/Blog/articles/how-to-build-evals-that-drive-impact/</guid><description>&lt;p>When you set up evaluation, you’re not just adding a score.&lt;/p>
&lt;p>You’re building a &lt;strong>system that decides what &amp;ldquo;good&amp;rdquo; looks like&lt;/strong>.&lt;/p>
&lt;p>Most teams don’t treat it that seriously. They build evals by convenience. Automate too early. Use generic metrics. Plug in LLM-as-a-Judge with a vague prompt and hope it works.&lt;/p>
&lt;p>It looks scientific, but in practice it’s just noise. Eventually they end up with an AI that does great on their internal benchmark but that none of their users really sticks to.&lt;/p></description></item><item><title>Where Are You Stuck? → 6 Questions to Diagnose your AI</title><link>https://louis-dupont.github.io/Blog/articles/where-are-you-actually-stuck/</link><pubDate>Fri, 28 Mar 2025 16:00:00 +0100</pubDate><guid>https://louis-dupont.github.io/Blog/articles/where-are-you-actually-stuck/</guid><description>&lt;p>You’ve built a chatbot. Maybe even launched it. You’ve made changes—prompt tweaks, retrieval tricks, some reranking logic. It’s probably better than it was. But is it &lt;em>really&lt;/em> getting better? Can you tell? Most teams can’t. Not because they’re bad. Because they don’t have the right visibility.&lt;/p>
&lt;p>That’s what this post is for.&lt;/p>
&lt;p>If you&amp;rsquo;re stuck, walk through these six questions. They’ll show you what’s missing.&lt;/p>
&lt;hr>
&lt;h3 id="1-do-you-have-logs">1. Do you have logs?&lt;/h3>
&lt;p>Can you open up a list of recent conversations and look at what users said, what the system answered, and how the flow went?&lt;/p></description></item><item><title>Stop Tweaking → Learn to Systematically Improve your AI</title><link>https://louis-dupont.github.io/Blog/articles/learn-to-systematically-improve-your-ai/</link><pubDate>Tue, 18 Mar 2025 15:42:04 +0100</pubDate><guid>https://louis-dupont.github.io/Blog/articles/learn-to-systematically-improve-your-ai/</guid><description>&lt;p>Many teams I&amp;rsquo;ve worked with start out making rapid progress with their AI - usually a RAG system. At first, iteration is fast. Each tweak makes a visible difference. But then, they hit a plateau.&lt;/p>
&lt;p>&lt;strong>Changes don&amp;rsquo;t seem to make a clear impact anymore&lt;/strong>. Some changes even backfire. The AI isn&amp;rsquo;t broken, but it&amp;rsquo;s no longer clear what to fix—or even how to tell if it&amp;rsquo;s improving at all.&lt;/p>
&lt;h2 id="when-metrics-stop-making-sense">When Metrics Stop Making Sense&lt;/h2>
&lt;p>That&amp;rsquo;s when most teams turn to metrics. And &lt;strong>that&amp;rsquo;s where things usually go wrong.&lt;/strong>&lt;/p></description></item><item><title>How to Fail your AI Project – Like I Did</title><link>https://louis-dupont.github.io/Blog/articles/how-to-fail-your-ai-project/</link><pubDate>Wed, 12 Mar 2025 15:59:10 +0100</pubDate><guid>https://louis-dupont.github.io/Blog/articles/how-to-fail-your-ai-project/</guid><description>&lt;p>A few years ago, I worked on my first Generative AI project: a customer-facing AI assistant. The company had unique customer data and believed AI could transform it into a highly personalized, valuable chatbot.&lt;/p>
&lt;p>We built a prototype fast. Our users were excited about the first demo.&lt;/p>
&lt;h2 id="our-first-big-mistake">Our First Big Mistake&lt;/h2>
&lt;p>But there was a catch: we had almost no access to real users. With so few user tests, we had to rely on ourselves—we became our own test users.&lt;/p></description></item></channel></rss>