<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLM Evals - The Trap No One’s Telling You 🐔 | Louis Dupont</title>
<meta name=keywords content><meta name=description content="We hear it more and more: ‘Use LLM Evaluations to guide your AI project.’ And for a good reason—metrics are essential.
Yet, there’s a trap nobody talks about&mldr;
Let’s say you have a chatbot and want to introduce metrics. You find tools that compute metrics like &lsquo;Helpfulness&rsquo;, &lsquo;Conciseness&rsquo;, and &lsquo;Completeness&rsquo;.
Sounds great—they promise to optimise your user’s experience. Right?
Truth is, their correlation to real business value is often unclear. Is this really what your user cares about ? Will this increase adoption ?"><meta name=author content="Louis Dupont"><link rel=canonical href=https://louis-dupont.github.io/Blog/articles/eval-trap/><link crossorigin=anonymous href=/Blog/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://louis-dupont.github.io/Blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://louis-dupont.github.io/Blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://louis-dupont.github.io/Blog/favicon-32x32.png><link rel=apple-touch-icon href=https://louis-dupont.github.io/Blog/apple-touch-icon.png><link rel=mask-icon href=https://louis-dupont.github.io/Blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://louis-dupont.github.io/Blog/articles/eval-trap/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-GX2HR4QZL5"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GX2HR4QZL5")}</script><meta property="og:url" content="https://louis-dupont.github.io/Blog/articles/eval-trap/"><meta property="og:site_name" content="Louis Dupont"><meta property="og:title" content="LLM Evals - The Trap No One’s Telling You 🐔"><meta property="og:description" content="We hear it more and more: ‘Use LLM Evaluations to guide your AI project.’ And for a good reason—metrics are essential.
Yet, there’s a trap nobody talks about…
Let’s say you have a chatbot and want to introduce metrics. You find tools that compute metrics like ‘Helpfulness’, ‘Conciseness’, and ‘Completeness’. Sounds great—they promise to optimise your user’s experience. Right?
Truth is, their correlation to real business value is often unclear. Is this really what your user cares about ? Will this increase adoption ?"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="articles"><meta property="article:published_time" content="2025-01-02T10:00:00+01:00"><meta property="article:modified_time" content="2025-01-02T10:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="LLM Evals - The Trap No One’s Telling You 🐔"><meta name=twitter:description content="We hear it more and more: ‘Use LLM Evaluations to guide your AI project.’ And for a good reason—metrics are essential.
Yet, there’s a trap nobody talks about&mldr;
Let’s say you have a chatbot and want to introduce metrics. You find tools that compute metrics like &lsquo;Helpfulness&rsquo;, &lsquo;Conciseness&rsquo;, and &lsquo;Completeness&rsquo;.
Sounds great—they promise to optimise your user’s experience. Right?
Truth is, their correlation to real business value is often unclear. Is this really what your user cares about ? Will this increase adoption ?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Latest Insights on AI \u0026 LLM Development","item":"https://louis-dupont.github.io/Blog/articles/"},{"@type":"ListItem","position":2,"name":"LLM Evals - The Trap No One’s Telling You 🐔","item":"https://louis-dupont.github.io/Blog/articles/eval-trap/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLM Evals - The Trap No One’s Telling You 🐔","name":"LLM Evals - The Trap No One’s Telling You 🐔","description":"We hear it more and more: ‘Use LLM Evaluations to guide your AI project.’ And for a good reason—metrics are essential.\nYet, there’s a trap nobody talks about\u0026hellip;\nLet’s say you have a chatbot and want to introduce metrics. You find tools that compute metrics like \u0026lsquo;Helpfulness\u0026rsquo;, \u0026lsquo;Conciseness\u0026rsquo;, and \u0026lsquo;Completeness\u0026rsquo;. Sounds great—they promise to optimise your user’s experience. Right?\nTruth is, their correlation to real business value is often unclear. Is this really what your user cares about ? Will this increase adoption ?\n","keywords":[],"articleBody":"We hear it more and more: ‘Use LLM Evaluations to guide your AI project.’ And for a good reason—metrics are essential.\nYet, there’s a trap nobody talks about…\nLet’s say you have a chatbot and want to introduce metrics. You find tools that compute metrics like ‘Helpfulness’, ‘Conciseness’, and ‘Completeness’. Sounds great—they promise to optimise your user’s experience. Right?\nTruth is, their correlation to real business value is often unclear. Is this really what your user cares about ? Will this increase adoption ?\nMany teams end up measuring the wrong thing, thinking they’re being data-driven, while forgetting about what really matters.\nMetrics aren’t inherently good. They’re only as useful as the questions they help you answer.\nIf you don’t ask ‘What does success look like?’ or ‘What is the goal I want to measure?’ your metrics aren’t leading you—they’re misleading you.\nSo, the next time you set metrics, ask yourself: Are you measuring what impacts your business goals—or just what’s easy to quantify?\nThe difference might explain why your AI project feels stuck.\nBecause chasing the wrong metrics isn’t progress. It’s running in circles—like a headless chicken.\n","wordCount":"188","inLanguage":"en","datePublished":"2025-01-02T10:00:00+01:00","dateModified":"2025-01-02T10:00:00+01:00","author":{"@type":"Person","name":"Louis Dupont"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://louis-dupont.github.io/Blog/articles/eval-trap/"},"publisher":{"@type":"Organization","name":"Louis Dupont","logo":{"@type":"ImageObject","url":"https://louis-dupont.github.io/Blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://louis-dupont.github.io/Blog/ accesskey=h title="Louis Dupont (Alt + H)">Louis Dupont</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://louis-dupont.github.io/Blog/ title=Home><span>Home</span></a></li><li><a href=https://louis-dupont.github.io/Blog/articles/ title=Articles><span>Articles</span></a></li><li><a href=https://louis-dupont.github.io/Blog/work-with-me/ title="Work with me"><span>Work with me</span></a></li></ul></nav></header><main class=main><article class=post-content><p style=display:none>Template: single.html</p><h1 class=post-title>LLM Evals - The Trap No One’s Telling You 🐔</h1><p class=post-description></p><div class=post-body><p>We hear it more and more: ‘Use LLM Evaluations to guide your AI project.’ And for a good reason—metrics are essential.</p><p><strong>Yet, there’s a trap nobody talks about&mldr;</strong></p><p>Let’s say you have a chatbot and want to introduce metrics. You find tools that compute metrics like &lsquo;Helpfulness&rsquo;, &lsquo;Conciseness&rsquo;, and &lsquo;Completeness&rsquo;.
Sounds great—they promise to optimise your user’s experience. Right?</p><p>Truth is, their correlation to real business value is often unclear. Is this really what your user cares about ? Will this increase adoption ?</p><p>Many teams end up measuring the wrong thing, thinking they’re being data-driven, while forgetting about what really matters.</p><p>Metrics aren’t inherently good. They’re only as useful as the questions they help you answer.</p><p><em>If you don’t ask ‘What does success look like?’ or ‘What is the goal I want to measure?’ your metrics aren’t leading you—they’re misleading you.</em></p><p>So, the next time you set metrics, ask yourself: Are you measuring what impacts your business goals—or just what’s easy to quantify?</p><p><strong>The difference might explain why your AI project feels stuck.</strong></p><p><em>Because chasing the wrong metrics isn’t progress. It’s running in circles—like a headless chicken.</em></p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://louis-dupont.github.io/Blog/>Louis Dupont</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>