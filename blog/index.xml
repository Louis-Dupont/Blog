<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blogs on Louis Dupont</title>
    <link>http://localhost:1313/Blog/blog/</link>
    <description>Recent content in Blogs on Louis Dupont</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Mar 2025 16:22:38 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/Blog/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ğŸª¤ The Chatbot Trap - Why Your LLM Project Is Stuck After the â€œWow Moment&#34;</title>
      <link>http://localhost:1313/Blog/blog/the-chatbot-trap/</link>
      <pubDate>Fri, 07 Mar 2025 16:22:38 +0100</pubDate>
      <guid>http://localhost:1313/Blog/blog/the-chatbot-trap/</guid>
      <description>&lt;p&gt;&lt;em&gt;Your LLM prototype amazed everyoneâ€”until it didnâ€™t. Now itâ€™s stuck, and no oneâ€™s using it. Hereâ€™s why.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When most companies experiment with AI, &lt;strong&gt;the go-to application is a chatbot&lt;/strong&gt;. Itâ€™s intuitive, it looks impressive, and it feels like magic. But hereâ€™s the cold, hard truth: &lt;strong&gt;chatbots are why most LLM projects fail.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Iâ€™ve seen it happen countless times. The team builds a chatbot to â€œharness AI,â€ and at first, it wows everyone. But then the cracks start to show:&lt;/p&gt;</description>
    </item>
    <item>
      <title>DO NOT use these LLM Metrics â›” And what to do instead!</title>
      <link>http://localhost:1313/Blog/blog/eval-metrics-to-avoid/</link>
      <pubDate>Sat, 15 Feb 2025 11:00:00 +0100</pubDate>
      <guid>http://localhost:1313/Blog/blog/eval-metrics-to-avoid/</guid>
      <description>&lt;p&gt;In two words: &lt;strong&gt;Generalist LLM metrics are more of a danger than an opportunity.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NEVER&lt;/strong&gt; start with them.&lt;/li&gt;
&lt;li&gt;Use them only as a &lt;strong&gt;last resort&lt;/strong&gt;â€”and even then, with strict guidelines!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;so-what-are-these-vague-generic-metrics&#34;&gt;So what are these vague, generic metrics?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Helpfulness&lt;/li&gt;
&lt;li&gt;Conciseness&lt;/li&gt;
&lt;li&gt;Tone&lt;/li&gt;
&lt;li&gt;Personalisation&lt;/li&gt;
&lt;li&gt;â€¦ and more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But whatâ€™s so wrong with them?&lt;/p&gt;
&lt;h2 id=&#34;these-metrics-lack-real-meaning&#34;&gt;These Metrics Lack Real Meaning&lt;/h2&gt;
&lt;p&gt;The biggest problem? Theyâ€™re designed to evaluate an &lt;strong&gt;LLM in general&lt;/strong&gt;, not a &lt;strong&gt;specific use case&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Error Analysis ğŸ”§ Stop Guessing, Start Fixing AI Models</title>
      <link>http://localhost:1313/Blog/blog/error-analys-intro/</link>
      <pubDate>Fri, 14 Feb 2025 12:00:00 +0100</pubDate>
      <guid>http://localhost:1313/Blog/blog/error-analys-intro/</guid>
      <description>&lt;p&gt;Error analysis is about digging deep into &lt;em&gt;why&lt;/em&gt; something isnâ€™t working - to learn from it. It might sound obvious, but it&amp;rsquo;s shockingly underused, especially where it matters most: AI development.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s explore what it is through an example&lt;/p&gt;
&lt;h2 id=&#34;cats-or-dogs-&#34;&gt;Cats or Dogs ?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;I&amp;rsquo;m skipping many details that may hurt Data Scientists for the sake of simplicity.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Say you have 200 images to classify as either cats or dogs. You build an AI and get &lt;strong&gt;78% accuracy&lt;/strong&gt; - not great. We need to do better. But how?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Evaluate your LLM! Ok, but what&#39;s next? ğŸ¤”</title>
      <link>http://localhost:1313/Blog/blog/evaluate-chatbot-whatnext/</link>
      <pubDate>Thu, 02 Jan 2025 10:00:00 +0100</pubDate>
      <guid>http://localhost:1313/Blog/blog/evaluate-chatbot-whatnext/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Everyone say you need to Evaluate your LLM. You just did it. Now what? ğŸ¤·â€â™‚ï¸&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You got a score. Great. Now, hereâ€™s the trap:&lt;/p&gt;
&lt;p&gt;You either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Trust it.&lt;/strong&gt; (&lt;em&gt;&amp;ldquo;Nice, let&amp;rsquo;s ship!&amp;rdquo;&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chase a better one.&lt;/strong&gt; (&lt;em&gt;&amp;ldquo;Tweak some stuff and re-run!&amp;rdquo;&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both are &lt;strong&gt;horrible ideas.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;step-1-stop-staring-at-numbers&#34;&gt;&lt;strong&gt;Step 1: Stop staring at numbers.&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Numbers feel scientific, but &lt;strong&gt;they lie all the time.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before doing anything, look at actual examples. &lt;strong&gt;Whatâ€™s failing?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bad output? &lt;strong&gt;Fix the model.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Good output but bad score? &lt;strong&gt;Fix the eval.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Both wrong? &lt;strong&gt;Youâ€™ve got bigger problems.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-2-solve-the-right-problem&#34;&gt;&lt;strong&gt;Step 2: Solve the right problem.&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If your &lt;strong&gt;model sucks&lt;/strong&gt;, tweak:&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM Evals - The Trap No Oneâ€™s Telling You ğŸ”</title>
      <link>http://localhost:1313/Blog/blog/eval-trap/</link>
      <pubDate>Thu, 02 Jan 2025 10:00:00 +0100</pubDate>
      <guid>http://localhost:1313/Blog/blog/eval-trap/</guid>
      <description>&lt;p&gt;We hear it more and more: â€˜Use LLM Evaluations to guide your AI project.â€™ And for a good reasonâ€”metrics are essential.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Yet, thereâ€™s a trap nobody talks about&amp;hellip;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Letâ€™s say you have a chatbot and want to introduce metrics. You find tools that compute metrics like &amp;lsquo;Helpfulness&amp;rsquo;, &amp;lsquo;Conciseness&amp;rsquo;, and &amp;lsquo;Completeness&amp;rsquo;.
Sounds greatâ€”they promise to optimise your userâ€™s experience. Right?&lt;/p&gt;
&lt;p&gt;Truth is, their correlation to real business value is often unclear. Is this really what your user cares about ? Will this increase adoption ?&lt;/p&gt;</description>
    </item>
    <item>
      <title>ğŸ“‰ Why Improving Your AI Model Is Killing Your Projectâ€™s Success</title>
      <link>http://localhost:1313/Blog/blog/dont-improve-ai/</link>
      <pubDate>Wed, 01 Jan 2025 16:22:38 +0100</pubDate>
      <guid>http://localhost:1313/Blog/blog/dont-improve-ai/</guid>
      <description>&lt;p&gt;&lt;em&gt;What if improving your AI model is the very thing holding your project back?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Youâ€™ve spent weeks fine-tuning itâ€”polishing every detail, boosting accuracy, solving edge cases. Yet, adoption hasnâ€™t moved. &lt;em&gt;Frustrating?&lt;/em&gt; Youâ€™re not aloneâ€”&lt;strong&gt;this is a trap many AI teams fall into.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The problem isnâ€™t that AI isnâ€™t ready. Itâ€™s that the way we approach AI makes us feel productive while ignoring the real challenge: solving critical user needs.&lt;/p&gt;
&lt;p&gt;Letâ€™s break down why this happensâ€”and how you can escape the trap.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Turn Your Broken Chatbot into Your Biggest Asset! ğŸ¦</title>
      <link>http://localhost:1313/Blog/blog/leverage-chatbot/</link>
      <pubDate>Sat, 28 Dec 2024 16:22:38 +0100</pubDate>
      <guid>http://localhost:1313/Blog/blog/leverage-chatbot/</guid>
      <description>&lt;p&gt;You launched your chatbot, andâ€¦ well, itâ€™s not going as planned. Users are confused, workflows feel disjointed, and your teamâ€™s enthusiasm is quickly waning. Sound familiar?&lt;/p&gt;
&lt;p&gt;Hereâ€™s the good news: your chatbot isnâ€™t just failingâ€”itâ€™s revealing what matters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Every awkward interaction or frustrated user is a clue&lt;/strong&gt;. The gaps in your botâ€™s performance mirror the gaps in your understanding of user needs. And those gaps? Theyâ€™re opportunities.&lt;/p&gt;
&lt;p&gt;In my &lt;a href=&#34;the-chatbot-trap.md&#34;&gt;recent post&lt;/a&gt;, I explained why so many AI projects fall shortâ€”teams jump straight into building chatbots without asking whether theyâ€™re the right solution for the problem at hand. Often, theyâ€™re not. But even a â€œfailingâ€ chatbot can become a powerful diagnostic tool for understanding user needs more deeply.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ğŸ¤·â€â™‚ï¸ ModernBERT Is Here - and Itâ€™s Not Just Another LLM Update</title>
      <link>http://localhost:1313/Blog/blog/modernbert/</link>
      <pubDate>Fri, 20 Dec 2024 16:22:38 +0100</pubDate>
      <guid>http://localhost:1313/Blog/blog/modernbert/</guid>
      <description>&lt;p&gt;BERT is back - and this time, itâ€™s &lt;strong&gt;faster, smarter, and built for the tasks that matter.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If youâ€™re working on &lt;strong&gt;retrieval&lt;/strong&gt;, &lt;strong&gt;classification&lt;/strong&gt;, or &lt;strong&gt;code search&lt;/strong&gt;, encoder models like BERT have likely been your go-to. Generative LLMs may grab headlines, but &lt;strong&gt;when it comes to focused, production-ready AI tasks, BERT still shines&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Earlier this year, I ran an experiment comparing models on a real-world taskâ€”analyzing product reviews. The results were eye-opening:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
