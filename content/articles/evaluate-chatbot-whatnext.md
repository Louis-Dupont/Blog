+++
date = '2025-01-02T10:00:00+01:00'
draft = false
title = "Evaluate your LLM! Ok, but what's next? ğŸ¤”"
+++

**Everyone say you need to Evaluate your LLM. You just did it. Now what? ğŸ¤·â€â™‚ï¸**

You got a score. Great. Now, hereâ€™s the trap:

You either:

- **Trust it.** (_"Nice, let's ship!"_)
- **Chase a better one.** (_"Tweak some stuff and re-run!"_)

Both are **horrible ideas.**

## **Step 1: Stop staring at numbers.**

Numbers feel scientific, but **they lie all the time.**

Before doing anything, look at actual examples. **Whatâ€™s failing?**

- Bad output? **Fix the model.**
- Good output but bad score? **Fix the eval.**
- Both wrong? **Youâ€™ve got bigger problems.**

## **Step 2: Solve the right problem.**

If your **model sucks**, tweak:

- Prompts
- Data retrieval
- Edge cases

If your **eval sucks**, rethink:

- Your scoring function
- What â€œgoodâ€ even means

## **Step 3: Iterate like a maniac.**

Change something â†’ Run eval â†’ Learn â†’ Repeat.

Basically, do [Error Analysis](error-analys-intro.md) on your Evals (instead of on your LLM)!

**Chasing numbers isnâ€™t progress.** Chasing the right insights is.
