# Why Most AI Teams Are Stuck

It took me way too long to realize this. If you're feeling stuck, here's why.

A few years ago, I worked on a Generative AI project, a customer-facing AI assistant. The company had **great data** and was convinced AI could turn it into something valuable.

We built a prototype fast. Users were excited.

Iteration was quick. Each tweak made the AI feel better.

Then we hit a wall.

We kept changing things, but‚Ä¶ **was it actually getting better? Or just different?**

We didn't know.

### **When "Iterating" Is Just Making Random Changes**

At first, improving the AI felt obvious. We spotted issues, fixed them, and saw real progress. But suddenly, **everything slowed down.**

- **Some changes made things better‚Äîbut we weren't sure why.**
- **Other changes made things worse‚Äîbut we couldn't explain how.**
- **Sometimes, things just felt‚Ä¶ different, not actually better.**

It took me way too long to realize: **we weren't iterating. We were guessing.**

We were tweaking prompts, adjusting retrieval parameters, fine-tuning the model‚Ä¶ but none of it was **measured.** We were just **testing on a few cherry-picked examples** and convincing ourselves that it felt better.

**And that's exactly how most AI teams get stuck.**

### **Better on a Few Examples Isn't Better**

When you're close to a project, it's easy to **think you can tell when something improves.** You run a few tests. The output looks better. So you assume progress.

But:

- **Did it actually improve across the board?**
- **Did it break something else in the process?**
- **Are you fixing what users actually care about‚Äîor just what you noticed?**

Most teams think they're iterating. **They're just moving.**

### **Iteration Without Measurement Is Just Movement**

And that's the real problem.

Most teams, when they hit this wall, do what we did: **try more things.**

- More prompt tweaks.
- More model adjustments.
- More retrieval fine-tuning.

But **without a way to measure progress**, this isn't iteration‚Äîit's just motion.

Real iteration isn't about making changes. **It's about knowing, at every step, whether those changes actually work.**

Without that, you're just **optimizing in the dark.**

### **So What's the Fix?**

The teams that move past this don't just **build better models**‚Äîthey build **better ways to measure what ‚Äúbetter‚Äù means.**

Instead of relying on gut feeling, they:

- **Define clear success criteria**‚Äîwhat actually makes an answer useful?
- **Measure changes systematically**‚Äînot just on a few cherry-picked examples.
- **Make sure improvements don't break what already works.**

### **The Bottom Line**

Most AI teams don't struggle to build AI. **They struggle to improve it.**

I learned this the hard way. But once I started treating iteration as something that needs **clear feedback loops, not gut feeling**, everything changed.

**If you're stuck in the loop of ‚Äútweaking but not improving,‚Äù you're not alone.**
Next, I'll break down **how to actually measure AI improvement‚Äîwithout getting trapped by misleading metrics.**

üëâ **Follow to get notified when it's out.**

üìå In the meantime, if you want to go deeper on AI iteration and continuous improvement, check out my **[Blog](https://louis-dupont.github.io/Blog/?utm_source=devto&utm_medium=article&utm_campaign=tech-ai&utm_content=article_001).**
